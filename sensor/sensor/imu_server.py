from flask import Flask, request
import json
import logging
import time 
import threading
from queue import Queue
from colorama import Fore, Style, init
import os
import sys

# ==============================================================================
# 1. Configuration and Global Variables
# ==============================================================================

init(autoreset=True) 
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')

app = Flask(__name__)

# Ù…Ø³ÛŒØ±Ù‡Ø§ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³ÛŒØ³ØªÙ… Ø´Ù…Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯
PROJECT_DIR = os.path.expanduser('~/Robo/Try3/sensor/sensor') 
DATA_DIR = os.path.join(PROJECT_DIR, "data")                    
OUTPUT_FILENAME = os.path.join(DATA_DIR, "imu_data.csv") # ğŸ‘ˆ Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø±Ø§ ØªØºÛŒÛŒØ± Ø¯Ø§Ø¯Ù…
PROCESSING_INTERVAL_SEC = 1.0 
IS_HEADER_WRITTEN = False 
data_queue = Queue() 

# Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø§ÛŒØ¬Ø§Ø¯ Ù¾ÙˆØ´Ù‡
try:
    os.makedirs(DATA_DIR, exist_ok=True)
    logging.info(f"{Fore.BLUE}Data directory confirmed: {DATA_DIR}")
except Exception as e:
    logging.error(f"{Fore.RED}Failed to create data directory {DATA_DIR}: {e}")
    sys.exit(1)

# ==============================================================================
# 2. Flask Route: Data Receiver (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
# ==============================================================================

@app.route('/data', methods=['POST'])
def receive_data():
    """
    Ø¯Ø±ÛŒØ§ÙØª Ø¨Ú† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ Ø§ÙØ²ÙˆØ¯Ù† Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ø¨Ú† Ùˆ Ø²Ù…Ø§Ù† Ø³Ø±ÙˆØ±ØŒ Ø³Ù¾Ø³ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØµÙ.
    """
    try:
        data_batch = request.json
        server_time = time.time() 

        if isinstance(data_batch, dict) and 'payload' in data_batch and isinstance(data_batch['payload'], list):
            sensor_list = data_batch['payload']
            
            metadata = {
                'server_received_timestamp': server_time, 
                'messageId': data_batch.get('messageId', ''),
                'sessionId': data_batch.get('sessionId', ''),
                'deviceId': data_batch.get('deviceId', '')
            }
            
            processed_batch = []
            for record in sensor_list:
                new_record = record.copy()
                new_record.update(metadata)
                processed_batch.append(new_record)

        else:
            logging.warning(f"{Fore.YELLOW}Received data does not contain a list under 'payload'. Skipping batch.")
            return 'Payload structure missing or incorrect', 400

        log_message = f"RECEIVED BATCH: MessageID={data_batch.get('messageId', 'N/A')}, Sensors={len(processed_batch)}"
        logging.info(f"{Fore.YELLOW}{log_message}")

        data_queue.put(processed_batch) 
        return 'OK', 200

    except json.JSONDecodeError:
        logging.error(f"{Fore.RED}Invalid JSON format received.")
        return 'Invalid JSON', 400
    except Exception as e:
        logging.error(f"{Fore.RED}Error receiving data: {e}", exc_info=True)
        return 'Internal Server Error', 500

# ----------------------------------------------------------------
# 3. Separate Thread for Data Processing and Saving (Ù…Ù†Ø·Ù‚ Ø¬Ø¯ÛŒØ¯)
# ----------------------------------------------------------------

def data_processor_thread():
    """
    Ø¯Ø±ÛŒØ§ÙØª Ø¨Ú†â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ø¨Ø§Ø´ØªÙ‡â€ŒØ´Ø¯Ù‡ØŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø§ÙˆÙ„ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù‡Ø± Ø³Ù†Ø³ÙˆØ± Ø¯Ø± Ø¢Ø®Ø±ÛŒÙ† Ø¨Ú†ØŒ 
    Ùˆ Ø°Ø®ÛŒØ±Ù‡ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÛŒÚ© Ø³Ø·Ø± Ø¯Ø± CSV Ù‡Ø± Û± Ø«Ø§Ù†ÛŒÙ‡.
    """
    global IS_HEADER_WRITTEN

    # ØªØ±ØªÛŒØ¨ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø«Ø§Ø¨Øª
    FIXED_METADATA_KEYS = [
        'sessionId', 
        'deviceId', 
        'messageId', 
        'server_received_timestamp', 
        # 'time' Ùˆ 'name' Ø§Ø² Ø§ÛŒÙ† Ù„ÛŒØ³Øª Ø­Ø°Ù Ø´Ø¯Ù†Ø¯ Ø²ÛŒØ±Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø³Ù†Ø³ÙˆØ± Ù…Ù‚Ø¯Ø§Ø± Ù…ØªÙØ§ÙˆØªÛŒ Ø¯Ø§Ø±Ù†Ø¯
        'time_of_latest_batch', # ÛŒÚ© Ø³ØªÙˆÙ† Ø²Ù…Ø§Ù† ÙˆØ§Ø­Ø¯ Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ø³Ø·Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
    ]
    
    logging.info(f"{Fore.CYAN}--- Data Processor Started (1 Hz, Single Row Mode) ---")
    
    while True:
        try:
            time.sleep(PROCESSING_INTERVAL_SEC)

            if data_queue.empty():
                continue

            # Ø¯Ø±ÛŒØ§ÙØª Ø¢Ø®Ø±ÛŒÙ† Ø¨Ú† Ø§Ù†Ø¨Ø§Ø´ØªÙ‡ Ø´Ø¯Ù‡
            latest_sensor_list = None
            server_time_of_latest_batch = None
            while not data_queue.empty():
                current_batch = data_queue.get_nowait()
                latest_sensor_list = current_batch
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø²Ù…Ø§Ù† Ø¯Ø±ÛŒØ§ÙØª Ø³Ø±ÙˆØ± Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† Ø±Ú©ÙˆØ±Ø¯ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø²Ù…Ø§Ù† Ú©Ù„ÛŒ Ø³Ø·Ø±
                if current_batch:
                    server_time_of_latest_batch = current_batch[0].get('server_received_timestamp')

            if latest_sensor_list is None:
                continue

            # --- Ù…Ù†Ø·Ù‚ ÙÛŒÙ„ØªØ±ÛŒÙ†Ú¯ Ø¬Ø¯ÛŒØ¯: ÙÙ‚Ø· Ø§ÙˆÙ„ÛŒÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ù‡Ø± Ø³Ù†Ø³ÙˆØ± ('name') ---
            selected_records_by_name = {}
            
            for record in latest_sensor_list:
                sensor_name = record.get('name')
                
                if sensor_name is None:
                    continue 

                # Ø§Ú¯Ø± Ù‚Ø¨Ù„Ø§Ù‹ ÛŒÚ© Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø² Ø§ÛŒÙ† Ø³Ù†Ø³ÙˆØ± Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ Ø¢Ù† Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†
                if sensor_name not in selected_records_by_name:
                    selected_records_by_name[sensor_name] = record
            
            records_to_save = selected_records_by_name.values()

            if not records_to_save:
                 logging.info(f"{Fore.YELLOW}No valid records found in the latest sensor list to save.")
                 continue 
            
            # --- Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªÙ…Ø§Ù… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¯Ø± ÛŒÚ© Ø³Ø·Ø± ÙˆØ§Ø­Ø¯ (Single Row) ---
            final_data_row = {}
            
            # 1. Ø§ÙØ²ÙˆØ¯Ù† Ù…ØªØ§Ø¯ÛŒØªØ§ÛŒ Ø«Ø§Ø¨Øª (Ø§Ø² Ø¢Ø®Ø±ÛŒÙ† Ø±Ú©ÙˆØ±Ø¯ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ)
            # Ø§Ø² Ø¢Ù†Ø¬Ø§ÛŒÛŒ Ú©Ù‡ Ù…ØªØ§Ø¯ÛŒØªØ§ (sessionId, deviceId, messageId) Ø¯Ø± Ú©Ù„ Ø¨Ú† Ø«Ø§Ø¨Øª Ø§Ø³ØªØŒ 
            # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒÙ… Ø§Ø² Ø§ÙˆÙ„ÛŒÙ† Ø±Ú©ÙˆØ±Ø¯ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒÙ….
            first_record = next(iter(records_to_save), None)
            if first_record:
                for k in FIXED_METADATA_KEYS:
                    if k == 'time_of_latest_batch':
                         # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø²Ù…Ø§Ù† Ø¯Ø±ÛŒØ§ÙØª Ø³Ø±ÙˆØ± Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø²Ù…Ø§Ù† Ø³Ø·Ø±
                         final_data_row[k] = str(server_time_of_latest_batch if server_time_of_latest_batch is not None else first_record.get('server_received_timestamp', ''))
                    else:
                        final_data_row[k] = str(first_record.get(k, ''))
            
            # 2. Ø§ÙØ²ÙˆØ¯Ù† ØªÙ…Ø§Ù… Ù…Ù‚Ø§Ø¯ÛŒØ± Ø³Ù†Ø³ÙˆØ±Ù‡Ø§ (Ø¨Ø§ Ù¾ÛŒØ´ÙˆÙ†Ø¯) Ùˆ Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ Ú©Ù„ÛŒØ¯Ù‡Ø§
            all_sensor_value_keys = set()
            for record in records_to_save:
                sensor_name = record.get('name')
                if not sensor_name: continue
                
                # Ø¨Ø±Ø§ÛŒ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ø²Ù…Ø§Ù† Ø³Ù†Ø³ÙˆØ± Ù‡Ù… Ù‚Ø§Ø¨Ù„ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø¨Ø§Ø´Ø¯
                final_data_row[f'{sensor_name}_time'] = str(record.get('time', ''))
                all_sensor_value_keys.add(f'{sensor_name}_time') 
                
                values_dict = record.get('values', {})
                for axis, value in values_dict.items():
                    prefixed_key = f"{sensor_name}_{axis}"
                    final_data_row[prefixed_key] = str(value)
                    all_sensor_value_keys.add(prefixed_key) 
            
            # ØªØ±Ú©ÛŒØ¨ Ú©Ù„ÛŒØ¯Ù‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø¯Ø±: Ø«Ø§Ø¨Øª + Ù…Ù‚Ø§Ø¯ÛŒØ± Ø³Ù†Ø³ÙˆØ±Ù‡Ø§
            final_keys = FIXED_METADATA_KEYS + sorted(list(all_sensor_value_keys)) 

            # --- Ø°Ø®ÛŒØ±Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± ÙØ§ÛŒÙ„ CSV ---
            with open(OUTPUT_FILENAME, 'a', encoding='utf-8') as f:
                if not IS_HEADER_WRITTEN:
                    f.write(",".join(final_keys) + "\n") 
                    IS_HEADER_WRITTEN = True
                
                # Ù†ÙˆØ´ØªÙ† Ø³Ø·Ø± Ú©Ø§Ù…Ù„
                values = [final_data_row.get(k, '') for k in final_keys] 
                f.write(",".join(values) + "\n")

                logging.info(f"{Fore.GREEN}PROCESSED & SAVED: Single row with {len(records_to_save)} sensors.")

        except Exception as e:
            logging.error(f"{Fore.RED}Error in data processor thread: {e}", exc_info=True)

# ----------------------------------------------------------------
# 4. Application Execution (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
# ----------------------------------------------------------------

if __name__ == '__main__':
    processor = threading.Thread(target=data_processor_thread, daemon=True)
    processor.start()

    logging.info("----------------------------------------------------------------")
    logging.info(f"{Fore.BLUE}Starting Flask server on http://0.0.0.0:5000/data ...")
    logging.info(f"{Fore.BLUE}Output File: {OUTPUT_FILENAME}")
    logging.info(f"{Fore.BLUE}Processing Rate: {1/PROCESSING_INTERVAL_SEC} Hz (Single Row Saving)")
    logging.info("----------------------------------------------------------------")
    
    app.run(host='0.0.0.0', port=5000, debug=True, use_reloader=False)